{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF0Dixh9uEQt7hJ8abeUOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohan-0521/Automated-Data-Cleaning-Tasks/blob/main/Data_Analysis_Task_Rohan_Angadi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Python for all the code given below. The code I have used for questions 1 and 2 are provided. At this point, due to the paucity of time, I had to stop. "
      ],
      "metadata": {
        "id": "O960t7an1GnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Task 1(i):"
      ],
      "metadata": {
        "id": "ngMCU4Nhsmgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code reads all the files with the specified file format in the filepath and then creates an empty variable in which it will store the merged dataframe. It extracts the date from the filename, reads the csv file, and adds in a column based on the date it extracted. Finally it concatenates all the columns and creates a new CSV which contains the merged dataframe and saves it in a file called emandi_clean.csv."
      ],
      "metadata": {
        "id": "PMj4nRhDsx80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path = r'C:\\Users\\rohan\\Documents\\Task 1'\n",
        "all_files = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "li = []  \n",
        "\n",
        "for filename in all_files:\n",
        "    # Extract the date from the filename\n",
        "    date = filename[-14:-4]\n",
        "    # Read the CSV file and add the date as a column\n",
        "    df = pd.read_csv(filename, header=3)\n",
        "    df['Date'] = date\n",
        "\n",
        "    # Select columns of interest\n",
        "    df = df[['mandiname3', 'Date', 'BranchName3', 'districtname3', 'LastArrival4', 'PreviousArrival4', 'todayarrival4', 'Progressivearrival4', 'TodayPurchase4', 'Progressivepurchase4', 'Textbox131', 'Unsold4', 'TodayLifting4', 'ProgressiveLifting4', 'Textbox139', 'MaximumPrice3', 'MinimumPrice3']]\n",
        "    # Append to the list of dataframes\n",
        "    li.append(df)\n",
        "\n",
        "# Concatenate all dataframes into a single one\n",
        "df_merged = pd.concat(li, axis=0, ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "df_merged.to_csv('emandi_clean.csv', index=False)\n"
      ],
      "metadata": {
        "id": "pKbZULJaspDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for task 1 (ii)"
      ],
      "metadata": {
        "id": "XW_gojpyyh14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('emandi_clean.csv')\n",
        "\n",
        "# Count the number of observations\n",
        "num_obs = df.shape[0]\n",
        "\n",
        "# Print the number of observations\n",
        "print(\"The number of observations in the emandi_clean.csv file is:\", num_obs)"
      ],
      "metadata": {
        "id": "Fd5k5Z6kyhLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of unique mandis are 2,936 and the number of dates for each mandi level observation are 38. This code returns the total number of observations which happens to be 111,568. "
      ],
      "metadata": {
        "id": "6tG-IfkIy84Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Task 1(iii)"
      ],
      "metadata": {
        "id": "5B8F9KtR0g_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Get the column names with missing values\n",
        "cols_with_missing = missing_values[missing_values > 0].index.tolist()\n",
        "\n",
        "# Print the column names with missing values\n",
        "print(\"The following columns have missing values:\", cols_with_missing)"
      ],
      "metadata": {
        "id": "zBJWPKcd0jmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code reports the names of the variables that have missing data or observations. The columns \"maximumprice\" and \"minimum price\" have missing observations."
      ],
      "metadata": {
        "id": "h8B9QSB90k6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Task 2(i):"
      ],
      "metadata": {
        "id": "ydVJQMk8tgWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    r\"C:\\Users\\rohan\\Documents\\New folder\\stata-task-rohan-0521-master\\input\\pb_emandi.csv\",\n",
        "    parse_dates=['dt'],\n",
        "    date_parser=lambda x: pd.to_datetime(x, format='%d%b%Y'),\n",
        "    dtype={'col11': str, 'col14': str}\n",
        ")\n",
        "\n",
        "# Filter the data for the specific date\n",
        "df = df[df['dt'].dt.strftime('%d-%m-%Y') == '08-05-2021']\n",
        "\n",
        "# Group the data by mandicode and get the maximum values for lastyeararrival and progressivearrival\n",
        "df_max = df.groupby(['mandicode']).agg({'lastyeararrival':'max', 'progressivearrival':'max'}).reset_index()\n",
        "\n",
        "# Create a new column to indicate whether the value in progressivearrival is greater or not\n",
        "df_max['this_year_vs_last'] = df_max.apply(lambda x: 'greater' if x['progressivearrival'] > x['lastyeararrival'] else 'smaller', axis=1)\n",
        "\n",
        "# Create a new CSV file with the required columns\n",
        "df_max[['mandicode', 'lastyeararrival', 'progressivearrival', 'this_year_vs_last']].to_csv('this_year_vs_last.csv', index=False)\n"
      ],
      "metadata": {
        "id": "HK0Qg1nuuqlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code thus far creates a new CSV file called this_year_vs_last.csv which contains data on the unique mandi, the valueof last years wheat receivable and this year's total as of the date 08-05-2021. It compares the values of interest in the two columns and then creates a new column called this_year_vs_last in which it displays \"greater\" or \"smaller\" depending on whether this year's total was larger than or smaller than last year's."
      ],
      "metadata": {
        "id": "yJ7WfASsusVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# open the CSV file\n",
        "with open(r'C:\\Users\\rohan\\Documents\\this_year_vs_last.csv', newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    \n",
        "    # initialize counters\n",
        "    total_rows = 0\n",
        "    greater_rows = 0\n",
        "    \n",
        "    # iterate over each row in the CSV file\n",
        "    for row in reader:\n",
        "        total_rows += 1\n",
        "        \n",
        "        # check if the value of the column \"this_year_vs_last\" contains the string \"greater\"\n",
        "        if \"greater\" in row[\"this_year_vs_last\"]:\n",
        "            greater_rows += 1\n",
        "    \n",
        "    # calculate the proportion of rows with the string \"greater\"\n",
        "    proportion = greater_rows / total_rows\n",
        "    \n",
        "    # print the proportion\n",
        "    print(f\"The proportion of rows with the string 'greater' is: {proportion:.2f}\")\n"
      ],
      "metadata": {
        "id": "wV-5PgI_vPrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this snippet of code goes through the newly created CSV file and counts the proportion of rows in which the output of the this_year_vs_last column is \"greater\". The value reported by the code is 44%. Which means that 44% of the mandis received a total amount of wheat larger than last year's over the whole procurement period."
      ],
      "metadata": {
        "id": "Y8O-TXJivUU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Task 2(ii)"
      ],
      "metadata": {
        "id": "dqYTknWXvpSk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhbOdKNKvsWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I was unclear about the merging task and was not able to proceed with it. I noticed a column called mandicode in the pb_farmers.csv document which contained the unique mandicode but no such identifier in the pb_emandi.csv file to aid the merging."
      ],
      "metadata": {
        "id": "8gnJvSdVxc0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Task 3 onwards: Omitted due to termination of alloted time for this exercise."
      ],
      "metadata": {
        "id": "I1YJ3sFixuia"
      }
    }
  ]
}